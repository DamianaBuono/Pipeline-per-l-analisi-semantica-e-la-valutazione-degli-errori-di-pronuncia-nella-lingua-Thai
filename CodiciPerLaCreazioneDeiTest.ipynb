{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamianaBuono/Pipeline-per-l-analisi-semantica-e-la-valutazione-degli-errori-di-pronuncia-nella-lingua-Thai/blob/main/CodiciPerLaCreazioneDeiTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NRHPW9ztNplW",
        "outputId": "22fb6634-930f-4038-fdaa-2c36ba1e9f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "a3cAzMSEQFRd",
        "outputId": "33a67384-eee6-4245-bc02-5ff09a512fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from gtts) (2.32.4)\n",
            "Collecting click<8.2,>=7.1 (from gtts)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gtts) (2025.8.3)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: click, gtts\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "Successfully installed click-8.1.8 gtts-2.5.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.1.2-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from pythainlp) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (2025.8.3)\n",
            "Downloading pythainlp-5.1.2-py3-none-any.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.1.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PackagePath' object has no attribute '_drv'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3991531762.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'apt-get install -y ffmpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sacremoses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pythainlp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[0;32m--> 958\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     }\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mparts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \"\"\"An object providing sequence-like access to the\n\u001b[1;32m    705\u001b[0m         components in the filesystem path.\"\"\"\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_load_parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install -q torchaudio transformers datasets soundfile\n",
        "!apt-get install -y -qq ffmpeg\n",
        "!pip install gtts\n",
        "!pip install torch torchaudio transformers librosa soundfile\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install sacremoses\n",
        "!pip install pythainlp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creazione del file di creazione del testset Per Tipolofìgia di errori inseriti"
      ],
      "metadata": {
        "id": "WuFqYc2OuXDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# === CONFIG ===\n",
        "DATASET_DIR = \"/content/drive/MyDrive/TesiMaggistrale/audiErrati/modello_finetunato_th_errori\"\n",
        "CSV_ORIGINALE = \"/content/drive/MyDrive/TesiMaggistrale/audiErrati/merged_dataset.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset.csv\"\n",
        "\n",
        "# --- Carica dataset preprocessato ---\n",
        "dataset = load_from_disk(DATASET_DIR)\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "# --- Carica CSV originale ---\n",
        "df_orig = pd.read_csv(CSV_ORIGINALE)\n",
        "\n",
        "# --- Ricrea audio_path per il testset ---\n",
        "# HuggingFace non salva più gli indici originali, quindi selezioniamo il test split usando la stessa logica di train_test_split\n",
        "# Supponiamo che tu abbia usato train_test_split con seed=42 e le stesse percentuali.\n",
        "# Qui prendiamo semplicemente le ultime righe del CSV (come esempio)\n",
        "# (devi adattare in base a come hai splittato)\n",
        "num_test = len(test_dataset)\n",
        "df_test_csv = df_orig.iloc[-num_test:][[\"audio_path\", \"trascrizione_originale\", \"trascrizione_errata\", \"dettagli_modifiche\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Converto test_dataset in DataFrame ---\n",
        "df_test_hf = test_dataset.to_pandas()\n",
        "\n",
        "# --- Allinea le colonne ---\n",
        "df_finale = pd.concat([df_test_hf.reset_index(drop=True), df_test_csv], axis=1)\n",
        "\n",
        "# --- Rinomina ---\n",
        "df_finale = df_finale.rename(columns={\"trascrizione_originale\": \"trascrizione_corretta\"})\n",
        "\n",
        "# --- Mantieni solo le colonne desiderate ---\n",
        "df_finale = df_finale[[\"input_values\", \"labels\", \"trascrizione_corretta\", \"trascrizione_errata\", \"dettagli_modifiche\", \"audio_path\"]]\n",
        "\n",
        "# --- Salva CSV ---\n",
        "df_finale.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "print(f\"✅ File di test salvato in: {OUTPUT_CSV}\")\n",
        "print(df_finale.head())\n"
      ],
      "metadata": {
        "id": "YyeJkYoEltl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import difflib\n",
        "import unicodedata\n",
        "\n",
        "# --- CONFIG PATH ---\n",
        "INPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset_updated_from_pairs.csv\"\n",
        "\n",
        "# --- MAPPINGS (ricavati dal tuo codice di injection) ---\n",
        "# tonal marks come li avevi:\n",
        "tones = ['่', '้', '๊', '๋']\n",
        "\n",
        "# costruisco i set di vocali e consonanti dalle mappe che mi hai mostrato\n",
        "pronunciation_confusions = {\n",
        "    'light': {\n",
        "        'consonants': {\n",
        "            'ด': ['ต'], 'ต': ['ด'],\n",
        "            'ร': ['ล'], 'ล': ['ร'],\n",
        "            'บ': ['ป'], 'ป': ['บ'],\n",
        "            'น': ['ม'], 'ม': ['น'],\n",
        "        },\n",
        "        'vowels': {\n",
        "            'า': ['ั'], 'ั': ['า'],\n",
        "            'เ': ['แ'], 'แ': ['เ'],\n",
        "            'ะ': ['า'], 'า': ['ะ'],\n",
        "        },\n",
        "    },\n",
        "    'heavy': {\n",
        "        'consonants': {\n",
        "            'บ': ['พ'], 'พ': ['บ'],\n",
        "            'ช': ['ซ'], 'ซ': ['ช'],\n",
        "            'ง': ['น'], 'น': ['ง'],\n",
        "            'ก': ['ข', 'ค'], 'ข': ['ก'], 'ค': ['ก'],\n",
        "            'ญ': ['ย'], 'ย': ['ญ'],\n",
        "        },\n",
        "        'vowels': {\n",
        "            'ิ': ['ี'], 'ี': ['ิ'],\n",
        "            'ุ': ['ู'], 'ู': ['ุ'],\n",
        "            'อ': ['โ'], 'โ': ['อ'],\n",
        "        },\n",
        "    },\n",
        "    'tones': tones\n",
        "}\n",
        "\n",
        "# costruzione set vowel e consonant includendo chiavi e valori\n",
        "vowel_set = set()\n",
        "for mode in ['light', 'heavy']:\n",
        "    vowel_dict = pronunciation_confusions[mode].get('vowels', {})\n",
        "    for k,vlist in vowel_dict.items():\n",
        "        vowel_set.add(k)\n",
        "        for v in vlist:\n",
        "            vowel_set.add(v)\n",
        "# aggiungo vocali thai comuni che possono comparire\n",
        "vowel_set.update(list(\"าเแะิีุูโ็ํ\"))  # espandibile se necessario\n",
        "\n",
        "consonant_set = set()\n",
        "for mode in ['light', 'heavy']:\n",
        "    cons_dict = pronunciation_confusions[mode].get('consonants', {})\n",
        "    for k,vlist in cons_dict.items():\n",
        "        consonant_set.add(k)\n",
        "        for v in vlist:\n",
        "            consonant_set.add(v)\n",
        "# aggiungo consonanti thai comuni (espandibile)\n",
        "consonant_set.update(list(\"กขคฆงจชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลวสหฬฮ\"))\n",
        "\n",
        "tone_set = set(tones)\n",
        "\n",
        "# funzione di utilità: normalizza le stringhe (NFC) per confronti consistenti\n",
        "def normalize_text(s):\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    return unicodedata.normalize('NFC', str(s))\n",
        "\n",
        "# funzione che classifica un singolo cambiamento (char o sequence) in T/V/C (o None)\n",
        "def classify_char_change(char):\n",
        "    if char == \"\":\n",
        "        return None\n",
        "    # char potrebbe essere più di un codice (es. segno di tono)\n",
        "    # controlliamo se contiene almeno un carattere di tono\n",
        "    if any(c in tone_set for c in char):\n",
        "        return 'TONO'\n",
        "    # se tutti i caratteri appartengono al vowel_set (o contiene almeno uno), consideriamo VOCALE\n",
        "    if any(c in vowel_set for c in char):\n",
        "        return 'VOCALE'\n",
        "    # se contiene consonanti\n",
        "    if any(c in consonant_set for c in char):\n",
        "        return 'CONSONANTE'\n",
        "    # fallback: None (non classificabile)\n",
        "    return None\n",
        "\n",
        "# funzione principale: prende due stringhe e ritorna set di tipi trovati\n",
        "def classify_changes_between_strings(ref, hyp):\n",
        "    \"\"\"\n",
        "    ref: trascrizione_corretta (stringa)\n",
        "    hyp: trascrizione_errata (stringa)\n",
        "    restituisce: set di tipi tra {'TONO','VOCALE','CONSONANTE'} (vuoto se nessuna)\n",
        "    \"\"\"\n",
        "    ref = normalize_text(ref)\n",
        "    hyp = normalize_text(hyp)\n",
        "    sm = difflib.SequenceMatcher(a=ref, b=hyp, autojunk=False)\n",
        "    types_found = set()\n",
        "\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        # tag in {'replace', 'delete', 'insert', 'equal'}\n",
        "        if tag == 'equal':\n",
        "            continue\n",
        "        # segmenti coinvolti\n",
        "        removed = ref[i1:i2]   # parte rimossa o sostituita\n",
        "        inserted = hyp[j1:j2]  # parte inserita o sostituita\n",
        "\n",
        "        # classify removed (deletion or from substitution)\n",
        "        t_removed = classify_char_change(removed)\n",
        "        t_inserted = classify_char_change(inserted)\n",
        "\n",
        "        if t_removed:\n",
        "            types_found.add(t_removed)\n",
        "        if t_inserted:\n",
        "            types_found.add(t_inserted)\n",
        "\n",
        "        # corner case: substitution where removed empty and inserted empty handled\n",
        "        # also handle case where removed/inserted are multiple char sequences:\n",
        "        # try to analyze per-char if whole span not classified\n",
        "        if not t_removed and removed:\n",
        "            for ch in removed:\n",
        "                tch = classify_char_change(ch)\n",
        "                if tch:\n",
        "                    types_found.add(tch)\n",
        "        if not t_inserted and inserted:\n",
        "            for ch in inserted:\n",
        "                tch = classify_char_change(ch)\n",
        "                if tch:\n",
        "                    types_found.add(tch)\n",
        "\n",
        "    return types_found\n",
        "\n",
        "# --- Caricamento CSV e applicazione ---\n",
        "df = pd.read_csv(INPUT_CSV, dtype=str)  # carichiamo come stringhe per sicurezza\n",
        "\n",
        "# Assicuriamoci che le colonne esistano\n",
        "required_cols = ['trascrizione_corretta', 'trascrizione_errata']\n",
        "for c in required_cols:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Colonna mancante nel CSV: {c}\")\n",
        "\n",
        "new_types = []\n",
        "for idx, row in df.iterrows():\n",
        "    ref = row['trascrizione_corretta']\n",
        "    hyp = row['trascrizione_errata']\n",
        "    types = classify_changes_between_strings(ref, hyp)\n",
        "    if len(types) == 0:\n",
        "        new_types.append('NESSUNA')\n",
        "    else:\n",
        "        # ordino per consistenza e concateno con ;\n",
        "        new_types.append(';'.join(sorted(types)))\n",
        "\n",
        "df['dettagli_modifiche'] = new_types\n",
        "\n",
        "# Salvo il CSV aggiornato\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
        "print(\"Salvato:\", OUTPUT_CSV)\n",
        "print(df[['trascrizione_corretta','trascrizione_errata','dettagli_modifiche']].head(12))\n"
      ],
      "metadata": {
        "id": "l9gXf0nFlwqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import pandas as pd\n",
        "import difflib\n",
        "import unicodedata\n",
        "\n",
        "# --- CONFIG ---\n",
        "INPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset_with_counts.csv\"\n",
        "\n",
        "# --- Set completi Thai ---\n",
        "thai_vowels = set('ะัาำิีึืฺุูเแโใไฤฦๅ')  # Vowel signs\n",
        "thai_consonants = set('กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผพฟภมยรฤลฦวศษสหฬอฮ')\n",
        "thai_tones = set('่้๊๋')  # Tone marks\n",
        "\n",
        "# --- Funzioni di supporto ---\n",
        "def classify_char(char):\n",
        "    if char in thai_tones:\n",
        "        return 'TONO'\n",
        "    elif char in thai_vowels:\n",
        "        return 'VOCALE'\n",
        "    elif char in thai_consonants:\n",
        "        return 'CONSONANTE'\n",
        "    return None\n",
        "\n",
        "def normalize_text(s):\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    return unicodedata.normalize('NFC', str(s)).replace(\" \", \"\")\n",
        "\n",
        "def count_changes_between_strings(ref, hyp):\n",
        "    ref = normalize_text(ref)\n",
        "    hyp = normalize_text(hyp)\n",
        "    sm = difflib.SequenceMatcher(a=ref, b=hyp, autojunk=False)\n",
        "    counts = {'TONO': 0, 'VOCALE': 0, 'CONSONANTE': 0}\n",
        "\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        removed = ref[i1:i2]\n",
        "        inserted = hyp[j1:j2]\n",
        "\n",
        "        if tag == 'equal':\n",
        "            continue\n",
        "        elif tag == 'replace':\n",
        "            # Conta tutti i caratteri sostituiti una sola volta\n",
        "            # allineando quelli corrispondenti\n",
        "            for r, c in zip(removed, inserted):\n",
        "                t_c = classify_char(c)\n",
        "                if t_c: counts[t_c] += 1\n",
        "            # Se ci sono caratteri extra in removed o inserted, contali\n",
        "            for r in removed[len(inserted):]:\n",
        "                t_r = classify_char(r)\n",
        "                if t_r: counts[t_r] += 1\n",
        "            for c in inserted[len(removed):]:\n",
        "                t_c = classify_char(c)\n",
        "                if t_c: counts[t_c] += 1\n",
        "        else:  # 'insert' o 'delete'\n",
        "            for r in removed:\n",
        "                t_r = classify_char(r)\n",
        "                if t_r: counts[t_r] += 1\n",
        "            for c in inserted:\n",
        "                t_c = classify_char(c)\n",
        "                if t_c: counts[t_c] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "# --- Carica CSV ---\n",
        "df = pd.read_csv(INPUT_CSV, dtype=str)\n",
        "\n",
        "# --- Controllo colonne necessarie ---\n",
        "required_cols = ['trascrizione_corretta', 'trascrizione_errata']\n",
        "for c in required_cols:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Colonna mancante: {c}\")\n",
        "\n",
        "# --- Calcolo tipi concatenati e conteggi ---\n",
        "concat_types = []\n",
        "tono_counts = []\n",
        "vocale_counts = []\n",
        "consonante_counts = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    counts = count_changes_between_strings(row['trascrizione_corretta'], row['trascrizione_errata'])\n",
        "    tipo_riga = [k for k, v in counts.items() if v > 0]\n",
        "    concat_types.append(';'.join(tipo_riga) if tipo_riga else 'NESSUNA')\n",
        "    tono_counts.append(counts['TONO'])\n",
        "    vocale_counts.append(counts['VOCALE'])\n",
        "    consonante_counts.append(counts['CONSONANTE'])\n",
        "\n",
        "df['dettagli_modifiche'] = concat_types\n",
        "df['TONO_count'] = tono_counts\n",
        "df['VOCALE_count'] = vocale_counts\n",
        "df['CONSONANTE_count'] = consonante_counts\n",
        "\n",
        "# --- Salva CSV aggiornato ---\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
        "print(\"✅ Salvato:\", OUTPUT_CSV)\n"
      ],
      "metadata": {
        "id": "gYpXjwhil1Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- CONFIG ---\n",
        "INPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset_with_counts.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/TesiMaggistrale/TestSuTipologia/\"\n",
        "\n",
        "# --- Carica CSV aggiornato ---\n",
        "df = pd.read_csv(INPUT_CSV, dtype=str)\n",
        "\n",
        "# --- Filtra per singolo tipo di errore ---\n",
        "def filter_single_type(df, tipo):\n",
        "    \"\"\"\n",
        "    Ritorna solo le righe dove dettagli_modifiche contiene il tipo specificato\n",
        "    e non contiene altri tipi.\n",
        "    \"\"\"\n",
        "    return df[df['dettagli_modifiche'] == tipo]\n",
        "\n",
        "# TONO singolo\n",
        "df_tono = filter_single_type(df, 'TONO').head(10)\n",
        "df_tono.to_csv(f\"{OUTPUT_DIR}testset_tono.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "# VOCALE singolo\n",
        "df_vocale = filter_single_type(df, 'VOCALE').head(10)\n",
        "df_vocale.to_csv(f\"{OUTPUT_DIR}testset_vocale.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "# CONSONANTE singolo\n",
        "df_consonante = filter_single_type(df, 'CONSONANTE').head(10)\n",
        "df_consonante.to_csv(f\"{OUTPUT_DIR}testset_consonante.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "print(\"✅ Generati testset separati per TONO, VOCALE e CONSONANTE.\")\n",
        "print(f\"- {len(df_tono)} frasi TONO\")\n",
        "print(f\"- {len(df_vocale)} frasi VOCALE\")\n",
        "print(f\"- {len(df_consonante)} frasi CONSONANTE\")\n"
      ],
      "metadata": {
        "id": "sXu1A-jxmLS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creazione testset per gli espermenti sulla **quantità di errori** introdotto"
      ],
      "metadata": {
        "id": "Vv64KcAFqa0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import difflib\n",
        "import unicodedata\n",
        "\n",
        "# --- CONFIG ---\n",
        "INPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset_with_counts.csv\"\n",
        "\n",
        "# --- Set completi Thai ---\n",
        "thai_vowels = set('ะัาำิีึืฺุูเแโใไฤฦๅ')  # Vowel signs\n",
        "thai_consonants = set('กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผพฟภมยรฤลฦวศษสหฬอฮ')\n",
        "thai_tones = set('่้๊๋')  # Tone marks\n",
        "\n",
        "# --- Funzioni di supporto ---\n",
        "def classify_char(char):\n",
        "    if char in thai_tones:\n",
        "        return 'TONO'\n",
        "    elif char in thai_vowels:\n",
        "        return 'VOCALE'\n",
        "    elif char in thai_consonants:\n",
        "        return 'CONSONANTE'\n",
        "    return None\n",
        "\n",
        "def normalize_text(s):\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    return unicodedata.normalize('NFC', str(s)).replace(\" \", \"\")\n",
        "\n",
        "def count_changes_between_strings(ref, hyp):\n",
        "    ref = normalize_text(ref)\n",
        "    hyp = normalize_text(hyp)\n",
        "    sm = difflib.SequenceMatcher(a=ref, b=hyp, autojunk=False)\n",
        "    counts = {'TONO': 0, 'VOCALE': 0, 'CONSONANTE': 0}\n",
        "\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        removed = ref[i1:i2]\n",
        "        inserted = hyp[j1:j2]\n",
        "\n",
        "        if tag == 'equal':\n",
        "            continue\n",
        "        elif tag == 'replace':\n",
        "            for r, c in zip(removed, inserted):\n",
        "                t_c = classify_char(c)\n",
        "                if t_c: counts[t_c] += 1\n",
        "            for r in removed[len(inserted):]:\n",
        "                t_r = classify_char(r)\n",
        "                if t_r: counts[t_r] += 1\n",
        "            for c in inserted[len(removed):]:\n",
        "                t_c = classify_char(c)\n",
        "                if t_c: counts[t_c] += 1\n",
        "        else:  # 'insert' o 'delete'\n",
        "            for r in removed:\n",
        "                t_r = classify_char(r)\n",
        "                if t_r: counts[t_r] += 1\n",
        "            for c in inserted:\n",
        "                t_c = classify_char(c)\n",
        "                if t_c: counts[t_c] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "# --- Carica CSV ---\n",
        "df = pd.read_csv(INPUT_CSV, dtype=str)\n",
        "\n",
        "# --- Controllo colonne necessarie ---\n",
        "required_cols = ['trascrizione_corretta', 'trascrizione_errata']\n",
        "for c in required_cols:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Colonna mancante: {c}\")\n",
        "\n",
        "# --- Calcolo tipi concatenati e conteggi ---\n",
        "concat_types = []\n",
        "tono_counts = []\n",
        "vocale_counts = []\n",
        "consonante_counts = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    counts = count_changes_between_strings(row['trascrizione_corretta'], row['trascrizione_errata'])\n",
        "    tipo_riga = [k for k, v in counts.items() if v > 0]\n",
        "    concat_types.append(';'.join(tipo_riga) if tipo_riga else 'NESSUNA')\n",
        "    tono_counts.append(counts['TONO'])\n",
        "    vocale_counts.append(counts['VOCALE'])\n",
        "    consonante_counts.append(counts['CONSONANTE'])\n",
        "\n",
        "df['dettagli_modifiche'] = concat_types\n",
        "df['TONO_count'] = tono_counts\n",
        "df['VOCALE_count'] = vocale_counts\n",
        "df['CONSONANTE_count'] = consonante_counts\n",
        "\n",
        "# --- Aggiungi colonna totale errori ---\n",
        "df['TOTALE_errori'] = df['TONO_count'] + df['VOCALE_count'] + df['CONSONANTE_count']\n",
        "\n",
        "# --- Salva CSV aggiornato ---\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
        "print(\"✅ Salvato con colonna TOTALE_errori:\", OUTPUT_CSV)\n"
      ],
      "metadata": {
        "id": "mtIEMwDrqZpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- CONFIG ---\n",
        "INPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/testeset_with_counts.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/TesiMaggistrale/TestSuTotaleErrori/\"\n",
        "\n",
        "# --- Carica CSV aggiornato ---\n",
        "df = pd.read_csv(INPUT_CSV, dtype=str)\n",
        "\n",
        "# Converti TOTALE_errori in intero per poter filtrare numericamente\n",
        "df['TOTALE_errori'] = df['TOTALE_errori'].astype(int)\n",
        "\n",
        "# --- Funzione per selezionare le prime 10 righe con un certo numero di errori ---\n",
        "def select_n_rows_by_total_errors(df, total_errors, n=10):\n",
        "    filtered = df[df['TOTALE_errori'] == total_errors].head(n)\n",
        "    return filtered\n",
        "\n",
        "# --- Genera file CSV per 1,2,3,4,5 errori totali ---\n",
        "for i in range(1, 6):\n",
        "    df_subset = select_n_rows_by_total_errors(df, i, n=10)\n",
        "    df_subset.to_csv(f\"{OUTPUT_DIR}testset_{i}_errori.csv\", index=False, encoding='utf-8')\n",
        "    print(f\"✅ {len(df_subset)} righe con {i} errore totale salvate in testset_{i}_errori.csv\")\n"
      ],
      "metadata": {
        "id": "53q_6c1hqpz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genera testset per 8,9 e 10 errori\n"
      ],
      "metadata": {
        "id": "bkqJIvQY6vvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp.corpus.wordnet import synsets\n",
        "\n",
        "# Assicura la presenza di WordNet\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "# Dizionario confusione esteso\n",
        "pronunciation_confusions = {\n",
        "    'light': {\n",
        "        'consonants': {\n",
        "            'ด': ['ต'], 'ต': ['ด'],\n",
        "            'ร': ['ล'], 'ล': ['ร'],\n",
        "            'บ': ['ป'], 'ป': ['บ'],\n",
        "            'น': ['ม'], 'ม': ['น'],\n",
        "        },\n",
        "        'vowels': {\n",
        "            'า': ['ั'], 'ั': ['า'],\n",
        "            'เ': ['แ'], 'แ': ['เ'],\n",
        "            'ะ': ['า'], 'า': ['ะ'],\n",
        "        },\n",
        "    },\n",
        "    'heavy': {\n",
        "        'consonants': {\n",
        "            'บ': ['พ'], 'พ': ['บ'],\n",
        "            'ช': ['ซ'], 'ซ': ['ช'],\n",
        "            'ง': ['น'], 'น': ['ง'],\n",
        "            'ก': ['ข', 'ค'], 'ข': ['ก'], 'ค': ['ก'],\n",
        "            'ญ': ['ย'], 'ย': ['ญ'],\n",
        "        },\n",
        "        'vowels': {\n",
        "            'ิ': ['ี'], 'ี': ['ิ'],\n",
        "            'ุ': ['ู'], 'ู': ['ุ'],\n",
        "            'อ': ['โ'], 'โ': ['อ'],\n",
        "        },\n",
        "    },\n",
        "    'tones': ['่', '้', '๊', '๋']\n",
        "}\n",
        "\n",
        "# combinazioni per fallback forzato\n",
        "combined_consonants = {}\n",
        "combined_vowels = {}\n",
        "for lvl in ['light', 'heavy']:\n",
        "    for k, v in pronunciation_confusions[lvl]['consonants'].items():\n",
        "        combined_consonants.setdefault(k, set()).update(v)\n",
        "    for k, v in pronunciation_confusions[lvl]['vowels'].items():\n",
        "        combined_vowels.setdefault(k, set()).update(v)\n",
        "combined_consonants = {k: list(v) for k, v in combined_consonants.items()}\n",
        "combined_vowels = {k: list(v) for k, v in combined_vowels.items()}\n",
        "\n",
        "thai_vocab = set(thai_words())\n",
        "tone_possible_chars = set(\"กขคฆงจชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลวสหฬอฮ\")\n",
        "\n",
        "\n",
        "def is_valid_lexical(word):\n",
        "    return word in thai_vocab\n",
        "\n",
        "\n",
        "def is_valid_semantic(word):\n",
        "    return len(synsets(word)) > 0\n",
        "\n",
        "\n",
        "def generate_pronunciation_variants(word, severity='light', max_steps=3):\n",
        "    consonants = pronunciation_confusions.get(severity, {}).get('consonants', {})\n",
        "    vowels = pronunciation_confusions.get(severity, {}).get('vowels', {})\n",
        "    tones = pronunciation_confusions['tones']\n",
        "\n",
        "    variants = set()\n",
        "\n",
        "    def recursive_modify(current_word, steps_left, changes):\n",
        "        if steps_left == 0:\n",
        "            return\n",
        "\n",
        "        for i, char in enumerate(current_word):\n",
        "            new_variants = []\n",
        "\n",
        "            # Sostituzione consonante\n",
        "            if char in consonants:\n",
        "                for rep in consonants[char]:\n",
        "                    mod_word = current_word[:i] + rep + current_word[i+1:]\n",
        "                    new_variants.append((mod_word, changes + [(char, rep, 'suono_consonante')]))\n",
        "\n",
        "            # Sostituzione vocale\n",
        "            if char in vowels:\n",
        "                for rep in vowels[char]:\n",
        "                    mod_word = current_word[:i] + rep + current_word[i+1:]\n",
        "                    new_variants.append((mod_word, changes + [(char, rep, 'suono_vocale')]))\n",
        "\n",
        "            # Inserimento di tono (dopo il carattere)\n",
        "            if char in tone_possible_chars:\n",
        "                for tone in tones:\n",
        "                    mod_word = current_word[:i+1] + tone + current_word[i+1:]\n",
        "                    new_variants.append((mod_word, changes + [('', tone, 'aggiunta_tono')]))\n",
        "\n",
        "            # Rimozione/sostituzione di vocale/tono\n",
        "            if char in list(vowels.keys()):\n",
        "                alt_chars = [v for v in vowels[char] if v != char]\n",
        "                for alt in alt_chars:\n",
        "                    mod_word = current_word[:i] + alt + current_word[i+1:]\n",
        "                    new_variants.append((mod_word, changes + [(char, alt, 'sostituzione_vocale')]))\n",
        "            if char in tones:\n",
        "                alt_chars = [t for t in tones if t != char]\n",
        "                for alt in alt_chars:\n",
        "                    mod_word = current_word[:i] + alt + current_word[i+1:]\n",
        "                    new_variants.append((mod_word, changes + [(char, alt, 'sostituzione_tono')]))\n",
        "\n",
        "            for variant, var_changes in new_variants:\n",
        "                if (variant, tuple(var_changes)) not in variants:\n",
        "                    variants.add((variant, tuple(var_changes)))\n",
        "                    recursive_modify(variant, steps_left - 1, var_changes)\n",
        "\n",
        "    recursive_modify(word, max_steps, [])\n",
        "    return list(variants)\n",
        "\n",
        "\n",
        "def force_modify_word(word):\n",
        "    if not word:\n",
        "        return word, []\n",
        "\n",
        "    i = random.randrange(len(word))\n",
        "    char = word[i]\n",
        "\n",
        "    if char in combined_consonants and random.random() < 0.6:\n",
        "        rep = random.choice(combined_consonants[char])\n",
        "        new_word = word[:i] + rep + word[i+1:]\n",
        "        return new_word, [(char, rep, 'forzata_sostituzione_consonante')]\n",
        "\n",
        "    if char in combined_vowels and random.random() < 0.6:\n",
        "        rep = random.choice(combined_vowels[char])\n",
        "        new_word = word[:i] + rep + word[i+1:]\n",
        "        return new_word, [(char, rep, 'forzata_sostituzione_vocale')]\n",
        "\n",
        "    if char in tone_possible_chars:\n",
        "        tone = random.choice(pronunciation_confusions['tones'])\n",
        "        new_word = word[:i+1] + tone + word[i+1:]\n",
        "        return new_word, [('', tone, 'forzata_aggiunta_tono')]\n",
        "\n",
        "    all_replacements = []\n",
        "    for lst in combined_consonants.values():\n",
        "        all_replacements.extend(lst)\n",
        "    for lst in combined_vowels.values():\n",
        "        all_replacements.extend(lst)\n",
        "    if all_replacements:\n",
        "        rep = random.choice(all_replacements)\n",
        "        new_word = word[:i] + rep + word[i+1:]\n",
        "        return new_word, [(char, rep, 'forzata_sostituzione_generica')]\n",
        "\n",
        "    return word, []\n",
        "\n",
        "\n",
        "def inject_pronunciation_error(word, severity='light', max_steps=3, require_valid=True, allow_force=False):\n",
        "    variants = generate_pronunciation_variants(word, severity=severity, max_steps=max_steps)\n",
        "\n",
        "    if require_valid:\n",
        "        valid_variants = [(w, list(changes)) for (w, changes) in variants\n",
        "                          if (is_valid_lexical(w) or is_valid_semantic(w)) and w != word]\n",
        "    else:\n",
        "        valid_variants = [(w, list(changes)) for (w, changes) in variants if w != word]\n",
        "\n",
        "    if valid_variants:\n",
        "        chosen_word, changes = random.choice(valid_variants)\n",
        "        return chosen_word, changes, True\n",
        "\n",
        "    if allow_force:\n",
        "        forced_word, forced_changes = force_modify_word(word)\n",
        "        if forced_word != word and forced_changes:\n",
        "            return forced_word, forced_changes, True\n",
        "\n",
        "    return word, [], False\n",
        "\n",
        "\n",
        "def inject_exact_num_errors(sentence, reference_sentence, target_errors=8,\n",
        "                            severity='light', require_valid=True, allow_force=True, max_per_word=None):\n",
        "    words = sentence.split()\n",
        "    new_words = words[:]\n",
        "    reference_words = reference_sentence.split()\n",
        "\n",
        "    errors_injected = 0\n",
        "    change_log = []\n",
        "    per_word_count = [0] * len(new_words)\n",
        "\n",
        "    stages = [\n",
        "        {'require_valid': require_valid, 'allow_force': False},\n",
        "        {'require_valid': False, 'allow_force': False},\n",
        "        {'require_valid': False, 'allow_force': allow_force},\n",
        "    ]\n",
        "\n",
        "    for stage in stages:\n",
        "        req = stage['require_valid']\n",
        "        af = stage['allow_force']\n",
        "\n",
        "        if errors_injected >= target_errors:\n",
        "            break\n",
        "\n",
        "        no_progress_rounds = 0\n",
        "        while errors_injected < target_errors and no_progress_rounds < 3:\n",
        "            indices = list(range(len(new_words)))\n",
        "            random.shuffle(indices)\n",
        "            progress_this_round = False\n",
        "\n",
        "            for i in indices:\n",
        "                if errors_injected >= target_errors:\n",
        "                    break\n",
        "\n",
        "                if max_per_word is not None and per_word_count[i] >= max_per_word:\n",
        "                    continue\n",
        "\n",
        "                original_word = new_words[i]\n",
        "                chosen_word, changes, applied = inject_pronunciation_error(\n",
        "                    original_word, severity=severity, max_steps=1, require_valid=req, allow_force=af\n",
        "                )\n",
        "\n",
        "                if applied and chosen_word != original_word and len(changes) > 0:\n",
        "                    change_log.append((i, original_word, chosen_word, changes))\n",
        "                    new_words[i] = chosen_word\n",
        "                    per_word_count[i] += len(changes)\n",
        "                    errors_injected += len(changes)\n",
        "                    progress_this_round = True\n",
        "\n",
        "                    if errors_injected >= target_errors:\n",
        "                        break\n",
        "\n",
        "            if not progress_this_round:\n",
        "                no_progress_rounds += 1\n",
        "            else:\n",
        "                no_progress_rounds = 0\n",
        "\n",
        "    if errors_injected > target_errors:\n",
        "        while errors_injected > target_errors and change_log:\n",
        "            idx, orig, neww, changes = change_log.pop()\n",
        "            new_words[idx] = orig\n",
        "            errors_injected -= len(changes)\n",
        "\n",
        "    success = (errors_injected == target_errors)\n",
        "    final_sentence = ' '.join(new_words[:len(reference_words)])\n",
        "\n",
        "    full_changes = []\n",
        "    for idx, orig, new_w, changes in change_log:\n",
        "        full_changes.append({\n",
        "            'index': idx,\n",
        "            'original': orig,\n",
        "            'modified': new_w,\n",
        "            'changes': changes\n",
        "        })\n",
        "\n",
        "    return final_sentence, full_changes, errors_injected, success\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    original_sentence = \"ฉัน รัก ภาษาไทย มาก มาก จริง ๆ วันนี้ สนุก\"\n",
        "    reference_sentence = original_sentence\n",
        "\n",
        "    # scegli a caso 8, 9 o 10\n",
        "    target = random.choice([8, 9, 10])\n",
        "\n",
        "    corrupted, error_log, injected_count, success = inject_exact_num_errors(\n",
        "        original_sentence,\n",
        "        reference_sentence,\n",
        "        target_errors=target,\n",
        "        severity='light',\n",
        "        require_valid=True,\n",
        "        allow_force=True,\n",
        "        max_per_word=1   # così una parola non viene modificata troppe volte\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Target scelto casualmente: {target}\")\n",
        "    print(\"Originale: \", original_sentence)\n",
        "    print(\"Corrotto:  \", corrupted)\n",
        "    print(f\"Errori iniettati: {injected_count}  Successo esatto?: {success}\")\n",
        "    print(\"--- Log dettagliato ---\")\n",
        "    if not error_log:\n",
        "        print(\"Nessuna modifica effettuata.\")\n",
        "    else:\n",
        "        for idx, entry in enumerate(error_log, 1):\n",
        "            i = entry['index']\n",
        "            print(f\"{idx}. index parola={i} '{entry['original']}' -> '{entry['modified']}'\")\n",
        "            for old, new_c, err_type in entry['changes']:\n",
        "                if old == '':\n",
        "                    print(f\"    + Aggiunto tono: '{new_c}' ({err_type})\")\n",
        "                elif new_c == '':\n",
        "                    print(f\"    - Rimosso '{old}' ({err_type})\")\n",
        "                else:\n",
        "                    print(f\"    x {err_type.upper()} - '{old}' → '{new_c}'\")\n"
      ],
      "metadata": {
        "id": "0wlec0nh62Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from transformers import pipeline\n",
        "from pythaitts import TTS\n",
        "\n",
        "# Percorsi\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/TesiMaggistrale/audiErrati/U/Office/Wav/UOM046_Pa046\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/TesiMaggistrale/audiErrati/Corpus_Injection_error_thai/\"\n",
        "CSV_PATH = os.path.join(OUTPUT_DIR, \"testseterrori.csv\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Modelli\n",
        "sr_pipe = pipeline(\"automatic-speech-recognition\", model=\"airesearch/wav2vec2-large-xlsr-53-th\")\n",
        "tts_model = TTS(pretrained=\"khanomtan\", version=\"1.0\", mode=\"best_model\")\n",
        "\n",
        "# Funzione per preparare audio per ASR\n",
        "def prepara_audio(path):\n",
        "    waveform, sr = torchaudio.load(path)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "    if sr != 16000:\n",
        "        waveform = T.Resample(sr, 16000)(waveform)\n",
        "    return waveform.squeeze().numpy()\n",
        "\n",
        "# Inizializza il CSV con intestazioni se non esiste\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    with open(CSV_PATH, mode=\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"filename\",\n",
        "            \"trascrizione_originale\",\n",
        "            \"trascrizione_errata\",\n",
        "            \"Totale_errori\",\n",
        "            \"modificata\",\n",
        "            \"dettagli_modifiche\",\n",
        "            \"audio_path\"\n",
        "        ])\n",
        "\n",
        "# Trova file audio\n",
        "audio_files = []\n",
        "for root, _, files in os.walk(AUDIO_DIR):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            audio_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"Trovati {len(audio_files)} file audio.\")\n",
        "\n",
        "# Loop sui file audio\n",
        "for audio_path in audio_files:\n",
        "    try:\n",
        "        base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "\n",
        "        # Step 1: ASR\n",
        "        audio_data = prepara_audio(audio_path)\n",
        "        trascrizione = sr_pipe(audio_data)[\"text\"].strip()\n",
        "        reference_sentence = trascrizione\n",
        "\n",
        "        # Step 2: Iniezione errori (assicura 8, 9 o 10)\n",
        "        target_errors = random.choice([8, 9, 10])\n",
        "        max_attempts = 10  # evita loop infinito\n",
        "        attempt = 0\n",
        "        success = False\n",
        "\n",
        "        while attempt < max_attempts and not success:\n",
        "            frase_errata, log_error, injected_count, success_flag = inject_exact_num_errors(\n",
        "                trascrizione,\n",
        "                reference_sentence,\n",
        "                target_errors=target_errors,\n",
        "                severity='light',\n",
        "                require_valid=True,\n",
        "                allow_force=True,\n",
        "                max_per_word=1\n",
        "            )\n",
        "            if injected_count == target_errors and success_flag:\n",
        "                success = True\n",
        "            else:\n",
        "                attempt += 1\n",
        "\n",
        "        if not success:\n",
        "            print(f\"⚠️ Attenzione: impossibile iniettare esattamente {target_errors} errori in {base_name}. Si procede comunque.\")\n",
        "\n",
        "        # Step 3: TTS\n",
        "        audio_out_path = os.path.join(OUTPUT_DIR, f\"{base_name}_corrupted.wav\")\n",
        "        tts_model.tts(\n",
        "            text=frase_errata,\n",
        "            speaker_idx=\"Tsyncone\",\n",
        "            language_idx=\"th-th\",\n",
        "            return_type=\"file\",\n",
        "            filename=audio_out_path\n",
        "        )\n",
        "\n",
        "        # Step 4: Log dettagliato modifiche\n",
        "        if not log_error:\n",
        "            dettagli = \"Nessuna modifica\"\n",
        "        else:\n",
        "            dettaglio_lista = []\n",
        "            for entry in log_error:\n",
        "                original = entry['original']\n",
        "                modified = entry['modified']\n",
        "                for old, new_c, tipo in entry['changes']:\n",
        "                    if old == '':\n",
        "                        dettaglio_lista.append(f\"Aggiunto tono '{new_c}' in '{modified}'\")\n",
        "                    elif new_c == '':\n",
        "                        dettaglio_lista.append(f\"Rimosso '{old}' da '{original}'\")\n",
        "                    else:\n",
        "                        dettaglio_lista.append(f\"{tipo.upper()} - '{original}': '{old}'→'{new_c}'\")\n",
        "            dettagli = \" | \".join(dettaglio_lista)\n",
        "\n",
        "        # Step 5: Salva nel CSV (7 colonne, ordine coerente)\n",
        "        with open(CSV_PATH, mode=\"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                base_name,\n",
        "                trascrizione,\n",
        "                frase_errata,\n",
        "                injected_count,             # Totale_errori PRIMA\n",
        "                \"SI\" if success else \"NO\",  # modificata\n",
        "                dettagli,\n",
        "                audio_out_path\n",
        "            ])\n",
        "\n",
        "        print(f\"✅ Salvato: {base_name} (target errori={target_errors}, effettivi={injected_count})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Errore su {audio_path}: {str(e)}\")\n",
        "\n",
        "print(\"✅ Creazione database completata.\")\n"
      ],
      "metadata": {
        "id": "Z64wUSe-7AIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torchaudio\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# === CONFIG ===\n",
        "CSV_PATH = \"/content/drive/MyDrive/TesiMaggistrale/audiErrati/Corpus_Injection_error_thai/testseterrori.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/audiErrati/Corpus_Injection_error_thai/testseterrori_preprocessed.csv\"\n",
        "SAMPLING_RATE = 16000\n",
        "PRETRAINED_MODEL_NAME = \"airesearch/wav2vec2-large-xlsr-53-th\"\n",
        "\n",
        "# === STEP 1: Carica CSV (tutte le colonne) ===\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Teniamo solo le righe con trascrizione_errata valida e file audio esistenti\n",
        "df = df.dropna(subset=[\"trascrizione_errata\"])\n",
        "df = df[df[\"audio_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "\n",
        "# === STEP 2: Carica processor ===\n",
        "processor = AutoProcessor.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "\n",
        "# === STEP 3: Funzione per estrarre input_values e labels ===\n",
        "def extract_features(row):\n",
        "    try:\n",
        "        waveform, sr = torchaudio.load(row[\"audio_path\"])\n",
        "\n",
        "        # Resample se necessario\n",
        "        if sr != SAMPLING_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLING_RATE)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Input values per il modello\n",
        "        input_values = processor(\n",
        "            waveform.squeeze().numpy(),\n",
        "            sampling_rate=SAMPLING_RATE\n",
        "        ).input_values[0]\n",
        "\n",
        "        # Labels (trascrizione tokenizzata)\n",
        "        with processor.as_target_processor():\n",
        "            labels = processor(row[\"trascrizione_errata\"]).input_ids\n",
        "\n",
        "        return pd.Series({\n",
        "            \"input_values\": input_values,\n",
        "            \"labels\": labels\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore su file {row['audio_path']}: {e}\")\n",
        "        return pd.Series({\"input_values\": None, \"labels\": None})\n",
        "\n",
        "# === STEP 4: Applica la funzione al DataFrame ===\n",
        "features = df.apply(extract_features, axis=1)\n",
        "df = pd.concat([features, df], axis=1)  # input_values e labels in testa\n",
        "\n",
        "# === STEP 5: Riordina le colonne ===\n",
        "ordered_cols = [\n",
        "    \"input_values\",\n",
        "    \"labels\",\n",
        "    \"trascrizione_originale\",\n",
        "    \"trascrizione_errata\",\n",
        "    \"dettagli_modifiche\",\n",
        "    \"audio_path\",\n",
        "    \"Totale_errori\"\n",
        "]\n",
        "df = df[ordered_cols]\n",
        "\n",
        "# === STEP 6: Salva il nuovo CSV ===\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(\"CSV preprocessato salvato in:\", OUTPUT_CSV)\n",
        "print(\"Righe valide:\", len(df))\n"
      ],
      "metadata": {
        "id": "2ITBZiqA7Ks_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- CONFIG ---\n",
        "INPUT_CSV = \"/content/drive/MyDrive/TesiMaggistrale/audiErrati/Corpus_Injection_error_thai/testseterrori.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/TesiMaggistrale/TestSuTotaleErrori/\"\n",
        "\n",
        "# --- Carica CSV ---\n",
        "df = pd.read_csv(INPUT_CSV, dtype=str)\n",
        "\n",
        "# --- Normalizza il nome della colonna ---\n",
        "if \"Totale_errori\" in df.columns:\n",
        "    df.rename(columns={\"Totale_errori\": \"TOTALE_errori\"}, inplace=True)\n",
        "\n",
        "# Converti in int per filtrare numericamente\n",
        "df[\"TOTALE_errori\"] = df[\"TOTALE_errori\"].astype(int)\n",
        "\n",
        "# --- Funzione per selezionare le prime n righe con un certo numero di errori ---\n",
        "def select_n_rows_by_total_errors(df, total_errors, n=10):\n",
        "    return df[df[\"TOTALE_errori\"] == total_errors].head(n)\n",
        "\n",
        "# --- Genera file CSV per un certo intervallo di errori ---\n",
        "for i in range(8, 11):  # 8, 9, 10 errori\n",
        "    df_subset = select_n_rows_by_total_errors(df, i, n=10)\n",
        "    df_subset.to_csv(f\"{OUTPUT_DIR}testset_{i}_errori.csv\", index=False, encoding=\"utf-8\")\n",
        "    print(f\"✅ {len(df_subset)} righe con {i} errori totali salvate in testset_{i}_errori.csv\")\n"
      ],
      "metadata": {
        "id": "e21hMLMo7N-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17gjwUv6RADznV7DT9x6wnusko5n481S2",
      "authorship_tag": "ABX9TyPKUBnqU88K8M8m2eJeYEMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}